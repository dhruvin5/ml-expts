{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "191070021_ML_Exp08.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL9HWdi83nQz"
      },
      "outputs": [],
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from random import random\n",
        "from csv import reader\n",
        "from math import exp, tanh\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFbGp0lkKqOU"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "  def __init__(self,learning_rate=0.01):\n",
        "    self.learning_rate = learning_rate\n",
        "    seed(10)\n",
        "\n",
        "  def initialize_network(self,n_inputs, n_hidden, n_outputs,n_layer):\n",
        "    network = []\n",
        "    for i in range(n_layer):\n",
        "      hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "      network.append(hidden_layer)\n",
        "    output_layer = [{'weights': [random() for i in range(n_hidden+ 1)]} for i in range(n_outputs)]\n",
        "    network.append(output_layer)\n",
        "    return network\n",
        "\n",
        "\n",
        "  def activate(self,weight,input):\n",
        "    total = weight[-1]\n",
        "    #print(weight,input)\n",
        "    for i in range(len(input)):\n",
        "      total += weight[i]*input[i]\n",
        "    return total\n",
        "\n",
        "  def sigmoid_activation(self,X):\n",
        "    return 1.0/(1 + np.exp(-X))\n",
        "\n",
        "  def sigmoid_derivative(self,X):\n",
        "    return X * (1.0 - X)\n",
        "\n",
        "  def forward_propagate(self,network, data_row ):\n",
        "\n",
        "    inputs= data_row\n",
        "\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        for neuron in layer:\n",
        "            activation = self.activate(neuron['weights'], inputs)\n",
        "            # Add the neuron output into the neuron item (before 'weight')\n",
        "            neuron['output'] = self.sigmoid_activation(activation)\n",
        "\n",
        "            new_inputs.append(neuron['output'])\n",
        "\n",
        "        inputs = new_inputs.copy()\n",
        "        # returns the outputs from the last layer\n",
        "    return inputs\n",
        "\n",
        "\n",
        "  def backward_propagate_error(self,network, expected):\n",
        "\n",
        "          # Start from last layer\n",
        "\n",
        "      for layer_index in reversed(range(len(network))):\n",
        "\n",
        "          layer = network[layer_index]\n",
        "          errors = []\n",
        "\n",
        "                  # --- (2) Error computed for the hidden layers: error = (weight_k * error_j) * transfer_derivative(output)\n",
        "\n",
        "          if layer_index != len(network) - 1:\n",
        "\n",
        "              for neuron_index in range(len(layer)):\n",
        "                  error = 0.0\n",
        "                  # --- (A) error = Sum(delta * weight linked to this delta)\n",
        "                  #                for each neuron[LAYER N+1] linked to this neuron[LAYER N] (current layer)\n",
        "                  for neuron_layer_M in network[layer_index + 1]:\n",
        "                      error += neuron_layer_M['weights'][neuron_index]  * neuron_layer_M['delta']\n",
        "                  errors.append(error)\n",
        "          else:\n",
        "                  # --- (1) Error computed for the last layer: error = (expected - output) * transfer_derivative(output)\n",
        "                          # --- (A) Store the difference between expected and output for each output neuron in errors[]\n",
        "              for neuron_index in range(len(layer)):\n",
        "                  neuron = layer[neuron_index]\n",
        "                  errors.append(expected[neuron_index] - neuron['output'])\n",
        "                  # --- (B) Store the error signal in delta for each neuron\n",
        "          for neuron_index in range(len(layer)):\n",
        "              neuron = layer[neuron_index]\n",
        "              neuron['delta'] = errors[neuron_index]* self.sigmoid_derivative(neuron['output'])\n",
        "\n",
        "\n",
        "  def update_weights(self,network, row):\n",
        "\n",
        "    for layer_index in range(len(network)):\n",
        "        inputs = row\n",
        "        if layer_index != 0:\n",
        "\n",
        "                        # --- (1) Store the outputs of the layer N-1 into inputs[]\n",
        "            inputs = [neuron['output'] for neuron in network[layer_index- 1]]\n",
        "\n",
        "        for neuron in network[layer_index]:\n",
        "\n",
        "                        # --- (2) Compute the new weights for each neuron of the layer N\n",
        "            for input_index in range(len(inputs)):\n",
        "                neuron['weights'][input_index] += self.learning_rate * neuron['delta'] * inputs[input_index]\n",
        "                        # --- (3) Update the bias of the neuron (input=1 below)\n",
        "\n",
        "            neuron['weights'][-1] += self.learning_rate * neuron['delta'] * 1\n",
        "\n",
        "\n",
        "# Training the network.\n",
        "\n",
        "  def train_network(self,network,train_data,train_out,test_data,test_out,n_epoch,n_outputs):\n",
        "      costs=[]\n",
        "      for epoch in range(n_epoch):\n",
        "          sum_error = 0\n",
        "          for row in range(len(train_data)):\n",
        "              outputs = self.forward_propagate(network, train_data[row])\n",
        "              expected = [train_out[row]]\n",
        "              sum_error += sum([(expected[i] - outputs[i]) ** 2 for i in\n",
        "                              range(len(expected))])\n",
        "              self.backward_propagate_error(network, expected)\n",
        "              self.update_weights(network, train_data[row])\n",
        "\n",
        "          curr_cost= sum_error/(2*len(train_data))\n",
        "          costs.append(curr_cost)\n",
        "\n",
        "          print(\"\\nAfter epoch {} the error and accuracy are \".format(epoch+1))\n",
        "          \n",
        "          curr_acc_test=self.get_prediction_accuracy(network, test_data,test_out)\n",
        "          curr_acc_train=self.get_prediction_accuracy(network, train_data,train_out)\n",
        "\n",
        "          print(\"Training Accuracy = \",curr_acc_train)\n",
        "          print(\"Test Accuracy = \",curr_acc_test)\n",
        "          print(\"Error = \",curr_cost)\n",
        "          print('\\n')\n",
        "\n",
        "      print(\"For Self Developed Code\")\n",
        "      print(\"Final Training Accuracy = \",curr_acc_train)\n",
        "      print(\"Final Test Accuracy = \",curr_acc_test)\n",
        "\n",
        "      return costs\n",
        "\n",
        "# Prediction with a network\n",
        "\n",
        "  def predict(self,network, row):\n",
        "      outputs = self.forward_propagate(network, row)\n",
        "      \n",
        "      if outputs[0]>=0.5:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "  def get_prediction_accuracy(self,network, test_data,test_out):\n",
        "      predictions = []\n",
        "      for row in test_data:\n",
        "          prediction = self.predict(network, row)\n",
        "          predictions.append(prediction)\n",
        "      expected_out = test_out\n",
        "      accuracy = accuracy_score(expected_out, predictions)\n",
        "      return accuracy\n",
        "\n",
        "# Backpropagation Algorithm\n",
        "\n",
        "  def back_propagation(self,train_data,train_out,test_data,test_out,n_epoch,n_hidden,n_layer):\n",
        "      n_inputs = len(train_data[0])\n",
        "      n_outputs = 1\n",
        "\n",
        "      network = self.initialize_network(n_inputs, n_hidden, n_outputs,n_layer)\n",
        "\n",
        "      layer = []\n",
        "      for i in range(len(network)):\n",
        "          layer.append(len(network[i]))\n",
        "      print('Network created with {} layers of sizes {} respectively\\n'.format(len(network), layer))\n",
        "\n",
        "      for i in range(len(network)):\n",
        "        for j in range(len(network[i])):\n",
        "          print(\"Weights initialized for layer {} node {} are {}\".format(i,j,np.around(network[i][j]['weights'],2)))\n",
        "\n",
        "      costs= self.train_network(network,train_data,train_out,test_data,test_out,n_epoch,n_outputs)\n",
        "      predictions = []\n",
        "\n",
        "      for row in test_data:\n",
        "          prediction = self.predict(network, row)\n",
        "          predictions.append(prediction)\n",
        "      return predictions,costs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('heart.csv')\n",
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "VLk6MTUe3Udv",
        "outputId": "99e941ef-6b31-4e2e-db48-1d850de06c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age          sex           cp     trestbps        chol  \\\n",
              "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.00000   \n",
              "mean     54.434146     0.695610     0.942439   131.611707   246.00000   \n",
              "std       9.072290     0.460373     1.029641    17.516718    51.59251   \n",
              "min      29.000000     0.000000     0.000000    94.000000   126.00000   \n",
              "25%      48.000000     0.000000     0.000000   120.000000   211.00000   \n",
              "50%      56.000000     1.000000     1.000000   130.000000   240.00000   \n",
              "75%      61.000000     1.000000     2.000000   140.000000   275.00000   \n",
              "max      77.000000     1.000000     3.000000   200.000000   564.00000   \n",
              "\n",
              "               fbs      restecg      thalach        exang      oldpeak  \\\n",
              "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.000000   \n",
              "mean      0.149268     0.529756   149.114146     0.336585     1.071512   \n",
              "std       0.356527     0.527878    23.005724     0.472772     1.175053   \n",
              "min       0.000000     0.000000    71.000000     0.000000     0.000000   \n",
              "25%       0.000000     0.000000   132.000000     0.000000     0.000000   \n",
              "50%       0.000000     1.000000   152.000000     0.000000     0.800000   \n",
              "75%       0.000000     1.000000   166.000000     1.000000     1.800000   \n",
              "max       1.000000     2.000000   202.000000     1.000000     6.200000   \n",
              "\n",
              "             slope           ca         thal       target  \n",
              "count  1025.000000  1025.000000  1025.000000  1025.000000  \n",
              "mean      1.385366     0.754146     2.323902     0.513171  \n",
              "std       0.617755     1.030798     0.620660     0.500070  \n",
              "min       0.000000     0.000000     0.000000     0.000000  \n",
              "25%       1.000000     0.000000     2.000000     0.000000  \n",
              "50%       1.000000     0.000000     2.000000     1.000000  \n",
              "75%       2.000000     1.000000     3.000000     1.000000  \n",
              "max       2.000000     4.000000     3.000000     1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c6e30b9-595c-4e44-bd69-69bec0edfe05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.00000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "      <td>1025.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.434146</td>\n",
              "      <td>0.695610</td>\n",
              "      <td>0.942439</td>\n",
              "      <td>131.611707</td>\n",
              "      <td>246.00000</td>\n",
              "      <td>0.149268</td>\n",
              "      <td>0.529756</td>\n",
              "      <td>149.114146</td>\n",
              "      <td>0.336585</td>\n",
              "      <td>1.071512</td>\n",
              "      <td>1.385366</td>\n",
              "      <td>0.754146</td>\n",
              "      <td>2.323902</td>\n",
              "      <td>0.513171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.072290</td>\n",
              "      <td>0.460373</td>\n",
              "      <td>1.029641</td>\n",
              "      <td>17.516718</td>\n",
              "      <td>51.59251</td>\n",
              "      <td>0.356527</td>\n",
              "      <td>0.527878</td>\n",
              "      <td>23.005724</td>\n",
              "      <td>0.472772</td>\n",
              "      <td>1.175053</td>\n",
              "      <td>0.617755</td>\n",
              "      <td>1.030798</td>\n",
              "      <td>0.620660</td>\n",
              "      <td>0.500070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>211.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>56.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>152.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>275.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.800000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c6e30b9-595c-4e44-bd69-69bec0edfe05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c6e30b9-595c-4e44-bd69-69bec0edfe05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c6e30b9-595c-4e44-bd69-69bec0edfe05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 0:-1].values\n",
        "Y = data.iloc[:, -1].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2,random_state=40)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "OYYSsjWf3w2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUL PREDICTION\n",
        "trainer=NeuralNetwork()\n",
        "n_epoch=100\n",
        "n_hidden=20\n",
        "n_layer = 1\n",
        "pred,costs=trainer.back_propagation(X_train,y_train,X_test,y_test,n_epoch,n_hidden,n_layer)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "cm = metrics.confusion_matrix(pred,y_test)\n",
        "print(\"\\nClassification report: \\n%s\\n\" % (metrics.classification_report(pred,y_test)))    \n",
        "print(\"Confusion matrix:\\n%s\" % cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ammPJc8z14zy",
        "outputId": "677c4917-5bec-42a1-d88f-2d3e13ee1f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network created with 2 layers of sizes [20, 1] respectively\n",
            "\n",
            "Weights initialized for layer 0 node 0 are [0.57 0.43 0.58 0.21 0.81 0.82 0.65 0.16 0.52 0.33 0.25 0.95 1.   0.04]\n",
            "Weights initialized for layer 0 node 1 are [0.86 0.6  0.38 0.28 0.67 0.46 0.69 0.66 0.13 0.77 0.98 0.97 0.61 0.04]\n",
            "Weights initialized for layer 0 node 2 are [0.   0.13 0.94 0.3  0.37 0.9  0.31 0.55 0.44 0.06 0.58 0.84 0.16 0.22]\n",
            "Weights initialized for layer 0 node 3 are [0.41 0.04 0.5  0.82 0.66 0.53 0.86 0.15 0.57 0.37 0.6  0.11 0.78 0.1 ]\n",
            "Weights initialized for layer 0 node 4 are [0.17 0.81 0.95 0.43 0.41 0.25 0.27 0.62 0.18 0.12 0.46 0.16 0.65 0.82]\n",
            "Weights initialized for layer 0 node 5 are [0.78 0.48 0.35 0.43 0.01 0.71 0.33 0.32 0.08 0.45 0.58 0.39 0.87 0.67]\n",
            "Weights initialized for layer 0 node 6 are [0.24 0.53 0.91 0.52 0.6  0.06 0.49 0.46 0.4  0.42 0.58 0.54 0.49 0.17]\n",
            "Weights initialized for layer 0 node 7 are [0.44 0.97 0.42 0.04 0.   0.54 0.05 0.09 0.11 0.45 0.99 0.49 0.46 0.44]\n",
            "Weights initialized for layer 0 node 8 are [0.5  0.45 0.73 0.9  0.7  0.42 0.65 0.91 0.15 0.24 0.62 0.69 0.64 0.59]\n",
            "Weights initialized for layer 0 node 9 are [0.83 0.48 0.81 0.96 0.44 0.86 0.72 0.89 0.05 0.91 0.77 0.96 0.98 0.29]\n",
            "Weights initialized for layer 0 node 10 are [0.67 0.14 0.6  0.92 0.19 0.35 0.08 0.19 0.21 0.1  1.   0.89 0.46 0.74]\n",
            "Weights initialized for layer 0 node 11 are [0.7  0.52 0.42 0.66 0.56 0.27 0.36 0.63 0.63 0.21 0.61 0.87 0.87 0.65]\n",
            "Weights initialized for layer 0 node 12 are [0.07 0.42 0.09 0.06 0.25 0.46 0.76 0.63 0.28 0.7  0.51 0.51 0.26 0.44]\n",
            "Weights initialized for layer 0 node 13 are [0.34 0.73 0.48 0.45 0.48 0.48 0.69 0.72 0.8  0.41 0.79 0.96 0.99 0.72]\n",
            "Weights initialized for layer 0 node 14 are [0.07 0.85 0.84 0.02 0.33 0.32 0.74 0.61 0.28 0.19 0.17 0.1  0.79 0.94]\n",
            "Weights initialized for layer 0 node 15 are [0.22 0.74 0.95 0.19 0.57 0.44 0.97 0.87 0.02 0.2  0.85 0.57 0.52 0.81]\n",
            "Weights initialized for layer 0 node 16 are [0.76 0.95 0.33 0.05 0.69 0.05 0.57 0.86 0.4  0.6  0.17 0.16 0.61 0.86]\n",
            "Weights initialized for layer 0 node 17 are [1.   0.03 0.92 0.51 0.35 0.4  0.58 0.35 0.15 0.87 0.71 0.61 0.72 0.99]\n",
            "Weights initialized for layer 0 node 18 are [0.18 0.82 0.82 0.34 0.56 0.46 0.19 0.43 0.15 0.93 0.19 0.63 0.57 0.97]\n",
            "Weights initialized for layer 0 node 19 are [0.79 0.36 0.38 0.05 0.05 0.31 0.95 0.05 0.18 0.64 0.36 0.19 0.95 0.04]\n",
            "Weights initialized for layer 1 node 0 are [0.6  0.43 0.35 0.19 0.16 0.41 0.75 0.47 0.78 0.88 0.17 0.88 0.58 0.42\n",
            " 0.51 0.89 0.39 0.09 0.96 0.12 0.11]\n",
            "\n",
            "After epoch 1 the error and accuracy are \n",
            "Training Accuracy =  0.5170731707317073\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.23442440196964476\n",
            "\n",
            "\n",
            "\n",
            "After epoch 2 the error and accuracy are \n",
            "Training Accuracy =  0.5170731707317073\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.23366560593143215\n",
            "\n",
            "\n",
            "\n",
            "After epoch 3 the error and accuracy are \n",
            "Training Accuracy =  0.5170731707317073\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.23277599292381532\n",
            "\n",
            "\n",
            "\n",
            "After epoch 4 the error and accuracy are \n",
            "Training Accuracy =  0.5170731707317073\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.2317136474568973\n",
            "\n",
            "\n",
            "\n",
            "After epoch 5 the error and accuracy are \n",
            "Training Accuracy =  0.5170731707317073\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.23041285686092908\n",
            "\n",
            "\n",
            "\n",
            "After epoch 6 the error and accuracy are \n",
            "Training Accuracy =  0.5134146341463415\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.22876306991140605\n",
            "\n",
            "\n",
            "\n",
            "After epoch 7 the error and accuracy are \n",
            "Training Accuracy =  0.5060975609756098\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.22656022910331994\n",
            "\n",
            "\n",
            "\n",
            "After epoch 8 the error and accuracy are \n",
            "Training Accuracy =  0.5060975609756098\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.2233754104105101\n",
            "\n",
            "\n",
            "\n",
            "After epoch 9 the error and accuracy are \n",
            "Training Accuracy =  0.4902439024390244\n",
            "Test Accuracy =  0.4975609756097561\n",
            "Error =  0.2181072080095524\n",
            "\n",
            "\n",
            "\n",
            "After epoch 10 the error and accuracy are \n",
            "Training Accuracy =  0.4951219512195122\n",
            "Test Accuracy =  0.5121951219512195\n",
            "Error =  0.20670079816103928\n",
            "\n",
            "\n",
            "\n",
            "After epoch 11 the error and accuracy are \n",
            "Training Accuracy =  0.5719512195121951\n",
            "Test Accuracy =  0.6390243902439025\n",
            "Error =  0.16333596779630832\n",
            "\n",
            "\n",
            "\n",
            "After epoch 12 the error and accuracy are \n",
            "Training Accuracy =  0.675609756097561\n",
            "Test Accuracy =  0.7560975609756098\n",
            "Error =  0.11435734265369837\n",
            "\n",
            "\n",
            "\n",
            "After epoch 13 the error and accuracy are \n",
            "Training Accuracy =  0.7012195121951219\n",
            "Test Accuracy =  0.8\n",
            "Error =  0.10516548368050084\n",
            "\n",
            "\n",
            "\n",
            "After epoch 14 the error and accuracy are \n",
            "Training Accuracy =  0.7060975609756097\n",
            "Test Accuracy =  0.8146341463414634\n",
            "Error =  0.09779931280079301\n",
            "\n",
            "\n",
            "\n",
            "After epoch 15 the error and accuracy are \n",
            "Training Accuracy =  0.7378048780487805\n",
            "Test Accuracy =  0.824390243902439\n",
            "Error =  0.09187547647320768\n",
            "\n",
            "\n",
            "\n",
            "After epoch 16 the error and accuracy are \n",
            "Training Accuracy =  0.7597560975609756\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.08709198765285306\n",
            "\n",
            "\n",
            "\n",
            "After epoch 17 the error and accuracy are \n",
            "Training Accuracy =  0.7597560975609756\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.08318997578916468\n",
            "\n",
            "\n",
            "\n",
            "After epoch 18 the error and accuracy are \n",
            "Training Accuracy =  0.774390243902439\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.07996656794842724\n",
            "\n",
            "\n",
            "\n",
            "After epoch 19 the error and accuracy are \n",
            "Training Accuracy =  0.7780487804878049\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.07726759210454327\n",
            "\n",
            "\n",
            "\n",
            "After epoch 20 the error and accuracy are \n",
            "Training Accuracy =  0.7939024390243903\n",
            "Test Accuracy =  0.8292682926829268\n",
            "Error =  0.074977345726633\n",
            "\n",
            "\n",
            "\n",
            "After epoch 21 the error and accuracy are \n",
            "Training Accuracy =  0.8048780487804879\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.07300929008028156\n",
            "\n",
            "\n",
            "\n",
            "After epoch 22 the error and accuracy are \n",
            "Training Accuracy =  0.8195121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.0712985531385822\n",
            "\n",
            "\n",
            "\n",
            "After epoch 23 the error and accuracy are \n",
            "Training Accuracy =  0.823170731707317\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.06979617066853432\n",
            "\n",
            "\n",
            "\n",
            "After epoch 24 the error and accuracy are \n",
            "Training Accuracy =  0.823170731707317\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.06846478274909454\n",
            "\n",
            "\n",
            "\n",
            "After epoch 25 the error and accuracy are \n",
            "Training Accuracy =  0.8341463414634146\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.0672754868262096\n",
            "\n",
            "\n",
            "\n",
            "After epoch 26 the error and accuracy are \n",
            "Training Accuracy =  0.8341463414634146\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.0662055769854816\n",
            "\n",
            "\n",
            "\n",
            "After epoch 27 the error and accuracy are \n",
            "Training Accuracy =  0.8378048780487805\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06523693982914924\n",
            "\n",
            "\n",
            "\n",
            "After epoch 28 the error and accuracy are \n",
            "Training Accuracy =  0.8414634146341463\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06435492163457877\n",
            "\n",
            "\n",
            "\n",
            "After epoch 29 the error and accuracy are \n",
            "Training Accuracy =  0.8414634146341463\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06354752465198334\n",
            "\n",
            "\n",
            "\n",
            "After epoch 30 the error and accuracy are \n",
            "Training Accuracy =  0.8402439024390244\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.06280482886658362\n",
            "\n",
            "\n",
            "\n",
            "After epoch 31 the error and accuracy are \n",
            "Training Accuracy =  0.8402439024390244\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.06211856700591403\n",
            "\n",
            "\n",
            "\n",
            "After epoch 32 the error and accuracy are \n",
            "Training Accuracy =  0.8378048780487805\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06148180432233495\n",
            "\n",
            "\n",
            "\n",
            "After epoch 33 the error and accuracy are \n",
            "Training Accuracy =  0.8378048780487805\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06088869139719572\n",
            "\n",
            "\n",
            "\n",
            "After epoch 34 the error and accuracy are \n",
            "Training Accuracy =  0.8378048780487805\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.06033426932459286\n",
            "\n",
            "\n",
            "\n",
            "After epoch 35 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.848780487804878\n",
            "Error =  0.05981431371719548\n",
            "\n",
            "\n",
            "\n",
            "After epoch 36 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05932520838821306\n",
            "\n",
            "\n",
            "\n",
            "After epoch 37 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.058863842301345684\n",
            "\n",
            "\n",
            "\n",
            "After epoch 38 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.0584275251096789\n",
            "\n",
            "\n",
            "\n",
            "After epoch 39 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.058013917736612554\n",
            "\n",
            "\n",
            "\n",
            "After epoch 40 the error and accuracy are \n",
            "Training Accuracy =  0.8426829268292683\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05762097522937194\n",
            "\n",
            "\n",
            "\n",
            "After epoch 41 the error and accuracy are \n",
            "Training Accuracy =  0.85\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.057246899676690985\n",
            "\n",
            "\n",
            "\n",
            "After epoch 42 the error and accuracy are \n",
            "Training Accuracy =  0.85\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05689010140495711\n",
            "\n",
            "\n",
            "\n",
            "After epoch 43 the error and accuracy are \n",
            "Training Accuracy =  0.85\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05654916699595207\n",
            "\n",
            "\n",
            "\n",
            "After epoch 44 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05622283293058767\n",
            "\n",
            "\n",
            "\n",
            "After epoch 45 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05590996387320845\n",
            "\n",
            "\n",
            "\n",
            "After epoch 46 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05560953478128197\n",
            "\n",
            "\n",
            "\n",
            "After epoch 47 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.055320616163902124\n",
            "\n",
            "\n",
            "\n",
            "After epoch 48 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.055042361925951175\n",
            "\n",
            "\n",
            "\n",
            "After epoch 49 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.054773999328260256\n",
            "\n",
            "\n",
            "\n",
            "After epoch 50 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.054514820671898244\n",
            "\n",
            "\n",
            "\n",
            "After epoch 51 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05426417638014045\n",
            "\n",
            "\n",
            "\n",
            "After epoch 52 the error and accuracy are \n",
            "Training Accuracy =  0.8548780487804878\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.054021469207230624\n",
            "\n",
            "\n",
            "\n",
            "After epoch 53 the error and accuracy are \n",
            "Training Accuracy =  0.8597560975609756\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05378614935056972\n",
            "\n",
            "\n",
            "\n",
            "After epoch 54 the error and accuracy are \n",
            "Training Accuracy =  0.8573170731707317\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.05355771028370956\n",
            "\n",
            "\n",
            "\n",
            "After epoch 55 the error and accuracy are \n",
            "Training Accuracy =  0.8536585365853658\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.05333568516236543\n",
            "\n",
            "\n",
            "\n",
            "After epoch 56 the error and accuracy are \n",
            "Training Accuracy =  0.8536585365853658\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.05311964368521878\n",
            "\n",
            "\n",
            "\n",
            "After epoch 57 the error and accuracy are \n",
            "Training Accuracy =  0.8597560975609756\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.052909189316081934\n",
            "\n",
            "\n",
            "\n",
            "After epoch 58 the error and accuracy are \n",
            "Training Accuracy =  0.8597560975609756\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.05270395679446662\n",
            "\n",
            "\n",
            "\n",
            "After epoch 59 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05250360987825368\n",
            "\n",
            "\n",
            "\n",
            "After epoch 60 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05230783927543275\n",
            "\n",
            "\n",
            "\n",
            "After epoch 61 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.0521163607322878\n",
            "\n",
            "\n",
            "\n",
            "After epoch 62 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.051928913253413526\n",
            "\n",
            "\n",
            "\n",
            "After epoch 63 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.05174525743500172\n",
            "\n",
            "\n",
            "\n",
            "After epoch 64 the error and accuracy are \n",
            "Training Accuracy =  0.8707317073170732\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.051565173897371114\n",
            "\n",
            "\n",
            "\n",
            "After epoch 65 the error and accuracy are \n",
            "Training Accuracy =  0.8682926829268293\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.05138846180604655\n",
            "\n",
            "\n",
            "\n",
            "After epoch 66 the error and accuracy are \n",
            "Training Accuracy =  0.8682926829268293\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.05121493747316543\n",
            "\n",
            "\n",
            "\n",
            "After epoch 67 the error and accuracy are \n",
            "Training Accuracy =  0.8682926829268293\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.051044433032803725\n",
            "\n",
            "\n",
            "\n",
            "After epoch 68 the error and accuracy are \n",
            "Training Accuracy =  0.8682926829268293\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.050876795185188346\n",
            "\n",
            "\n",
            "\n",
            "After epoch 69 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.050711884005819025\n",
            "\n",
            "\n",
            "\n",
            "After epoch 70 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.0505495718163687\n",
            "\n",
            "\n",
            "\n",
            "After epoch 71 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.05038974211493128\n",
            "\n",
            "\n",
            "\n",
            "After epoch 72 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.05023228856377793\n",
            "\n",
            "\n",
            "\n",
            "After epoch 73 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.05007711403328908\n",
            "\n",
            "\n",
            "\n",
            "After epoch 74 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.049924129701152134\n",
            "\n",
            "\n",
            "\n",
            "After epoch 75 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.049773254206257715\n",
            "\n",
            "\n",
            "\n",
            "After epoch 76 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04962441285697042\n",
            "\n",
            "\n",
            "\n",
            "After epoch 77 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04947753689360481\n",
            "\n",
            "\n",
            "\n",
            "After epoch 78 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.049332562804979496\n",
            "\n",
            "\n",
            "\n",
            "After epoch 79 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.049189431698859684\n",
            "\n",
            "\n",
            "\n",
            "After epoch 80 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04904808872594309\n",
            "\n",
            "\n",
            "\n",
            "After epoch 81 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04890848255678815\n",
            "\n",
            "\n",
            "\n",
            "After epoch 82 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.0487705649107657\n",
            "\n",
            "\n",
            "\n",
            "After epoch 83 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04863429013573551\n",
            "\n",
            "\n",
            "\n",
            "After epoch 84 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04849961483675103\n",
            "\n",
            "\n",
            "\n",
            "After epoch 85 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04836649755168396\n",
            "\n",
            "\n",
            "\n",
            "After epoch 86 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04823489847127664\n",
            "\n",
            "\n",
            "\n",
            "After epoch 87 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.04810477920078409\n",
            "\n",
            "\n",
            "\n",
            "After epoch 88 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.047976102560079704\n",
            "\n",
            "\n",
            "\n",
            "After epoch 89 the error and accuracy are \n",
            "Training Accuracy =  0.8695121951219512\n",
            "Test Accuracy =  0.8341463414634146\n",
            "Error =  0.047848832418880424\n",
            "\n",
            "\n",
            "\n",
            "After epoch 90 the error and accuracy are \n",
            "Training Accuracy =  0.8719512195121951\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.0477229335636128\n",
            "\n",
            "\n",
            "\n",
            "After epoch 91 the error and accuracy are \n",
            "Training Accuracy =  0.8719512195121951\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.04759837159237324\n",
            "\n",
            "\n",
            "\n",
            "After epoch 92 the error and accuracy are \n",
            "Training Accuracy =  0.8719512195121951\n",
            "Test Accuracy =  0.8390243902439024\n",
            "Error =  0.04747511283446278\n",
            "\n",
            "\n",
            "\n",
            "After epoch 93 the error and accuracy are \n",
            "Training Accuracy =  0.874390243902439\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.047353124291053254\n",
            "\n",
            "\n",
            "\n",
            "After epoch 94 the error and accuracy are \n",
            "Training Accuracy =  0.874390243902439\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.0472323735936992\n",
            "\n",
            "\n",
            "\n",
            "After epoch 95 the error and accuracy are \n",
            "Training Accuracy =  0.874390243902439\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.04711282897760248\n",
            "\n",
            "\n",
            "\n",
            "After epoch 96 the error and accuracy are \n",
            "Training Accuracy =  0.874390243902439\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.04699445926677721\n",
            "\n",
            "\n",
            "\n",
            "After epoch 97 the error and accuracy are \n",
            "Training Accuracy =  0.874390243902439\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.046877233868522064\n",
            "\n",
            "\n",
            "\n",
            "After epoch 98 the error and accuracy are \n",
            "Training Accuracy =  0.8780487804878049\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.04676112277489197\n",
            "\n",
            "\n",
            "\n",
            "After epoch 99 the error and accuracy are \n",
            "Training Accuracy =  0.8780487804878049\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.046646096569140046\n",
            "\n",
            "\n",
            "\n",
            "After epoch 100 the error and accuracy are \n",
            "Training Accuracy =  0.8780487804878049\n",
            "Test Accuracy =  0.8439024390243902\n",
            "Error =  0.04653212643538609\n",
            "\n",
            "\n",
            "For Self Developed Code\n",
            "Final Training Accuracy =  0.8780487804878049\n",
            "Final Test Accuracy =  0.8439024390243902\n",
            "\n",
            "Classification report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84        99\n",
            "           1       0.86      0.83      0.85       106\n",
            "\n",
            "    accuracy                           0.84       205\n",
            "   macro avg       0.84      0.84      0.84       205\n",
            "weighted avg       0.84      0.84      0.84       205\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[85 14]\n",
            " [18 88]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARY PREDICTION\n",
        "clf = MLPClassifier(activation='logistic').fit(X_train, y_train)\n",
        "y_pred=clf.predict(X_test)\n",
        "y_pred_train=clf.predict(X_train)\n",
        "\n",
        "print(\"\\nFor Library\")\n",
        "print(\"Train Accuracy=\",accuracy_score(y_train,y_pred_train))\n",
        "print(\"Test Accuracy=\",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "r0XFoJBi4103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d72c114-322a-4c3e-c28d-186c3b3b2bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For Library\n",
            "Train Accuracy= 0.8634146341463415\n",
            "Test Accuracy= 0.8195121951219512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "th=  np.array([i for i in range(n_epoch)])\n",
        "cost= np.array(costs)\n",
        "plt.title('Lost function of training data set')\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.plot(th, cost)\n",
        "print(\"\\n\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CWjb1JnW45d-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "3b348379-8fe7-4a1a-f563-2fbbc8aaf36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}